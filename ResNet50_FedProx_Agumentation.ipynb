{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86168368-5c79-4343-873e-ba99c017d91d",
   "metadata": {
    "id": "86168368-5c79-4343-873e-ba99c017d91d"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea113325-87fd-4cd7-9c28-20ea27da6e08",
   "metadata": {
    "id": "ea113325-87fd-4cd7-9c28-20ea27da6e08"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "563ce789-ce43-4e83-a808-b3e0f877da90",
   "metadata": {
    "id": "563ce789-ce43-4e83-a808-b3e0f877da90",
    "outputId": "0f80e2cc-fdb2-40ad-b749-91559abed246"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 13:45:21.098703: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-04 13:45:22.222057: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Flatten, Input, BatchNormalization, Dense, Activation, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "267b2aa7-246e-4c4c-92f6-0b1a258ceab1",
   "metadata": {
    "id": "267b2aa7-246e-4c4c-92f6-0b1a258ceab1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da37f179-0ae1-4fe1-81b3-fe73c0e11b38",
   "metadata": {
    "id": "da37f179-0ae1-4fe1-81b3-fe73c0e11b38"
   },
   "outputs": [],
   "source": [
    "# # Define paths\n",
    "# dataset_path = 'BrainTumor_MRI/Training'\n",
    "# class_names = ['glioma', 'meningioma', 'notumor', 'pituitary']  # Assuming the subfolders are named accordingly\n",
    "\n",
    "# # Parameters\n",
    "# IMG_SIZE = 256\n",
    "\n",
    "\n",
    "# # Plot the training and testing metrics\n",
    "# plot_acc_loss(\"FedProx Training and Testing Metrics for mu=1.0\",\n",
    "#               train_loss_hist_4, train_acc_hist_4, test_loss_hist_4, test_acc_hist_4)\n",
    "\n",
    "# # File to save the results\n",
    "# output_file = \"results_mu=1.0.txt\"\n",
    "# # Save the history to a file\n",
    "# save_history_to_file(output_file, n_iter, train_loss_hist_4, train_acc_hist_4, test_loss_hist_4, test_acc_hist_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ddd02-c071-4f39-8937-04a11f6ad465",
   "metadata": {
    "id": "3b7ddd02-c071-4f39-8937-04a11f6ad465"
   },
   "source": [
    "# https://chatgpt.com/share/670f84c9-a108-8000-a415-7248537697dc  - to get explanation related to file format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "asYWGGEXUXtQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "asYWGGEXUXtQ",
    "outputId": "a1ba1e89-09de-4238-e7c8-acb9ba8a3427"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define paths for training and testing datasets\n",
    "source_train_dir = \"BrainTumor_MRI/Training\"\n",
    "source_test_dir = \"BrainTumor_MRI/Testing\"\n",
    "\n",
    "# Define class names\n",
    "classes = [\"glioma\", \"meningioma\", \"notumor\", \"pituitary\"]\n",
    "\n",
    "# Check if images are in the correct source directory\n",
    "for class_name in classes:\n",
    "    class_train_folder = os.path.join(source_train_dir, class_name)\n",
    "    class_test_folder = os.path.join(source_test_dir, class_name)\n",
    "\n",
    "    if not os.path.exists(class_train_folder):\n",
    "        print(f\"Source training folder for {class_name} does not exist!\")\n",
    "    if not os.path.exists(class_test_folder):\n",
    "        print(f\"Source testing folder for {class_name} does not exist!\")\n",
    "    else:\n",
    "        print(f\"Source folders exist and contain {len(os.listdir(class_train_folder))} training images and {len(os.listdir(class_test_folder))} testing images for {class_name}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "U8-2ah-CgGHW",
   "metadata": {
    "id": "U8-2ah-CgGHW"
   },
   "outputs": [],
   "source": [
    "# ---------------- CBAM Block Definition ------------------\n",
    "def cbam_block(inputs, reduction_ratio=0.5):\n",
    "    channels = inputs.shape[-1]\n",
    "\n",
    "    # Channel attention\n",
    "    avg_pool = layers.GlobalAveragePooling2D()(inputs)\n",
    "    max_pool = layers.GlobalMaxPooling2D()(inputs)\n",
    "    shared_layer_1 = layers.Dense(int(channels * reduction_ratio), activation='relu', use_bias=True)\n",
    "    shared_layer_2 = layers.Dense(channels, activation='relu', use_bias=True)\n",
    "\n",
    "    avg_pool = shared_layer_1(avg_pool)\n",
    "    avg_pool = shared_layer_2(avg_pool)\n",
    "    max_pool = shared_layer_1(max_pool)\n",
    "    max_pool = shared_layer_2(max_pool)\n",
    "\n",
    "    attention = layers.Add()([avg_pool, max_pool])\n",
    "    attention = layers.Activation('sigmoid')(attention)\n",
    "    attention = layers.Reshape((1, 1, channels))(attention)\n",
    "    scaled_inputs = layers.Multiply()([inputs, attention])\n",
    "\n",
    "    # Spatial attention\n",
    "    squeeze = layers.Conv2D(filters=1, kernel_size=1, activation='sigmoid', use_bias=False)(scaled_inputs)\n",
    "    expanded_inputs = layers.Multiply()([scaled_inputs, squeeze])\n",
    "\n",
    "    return expanded_inputs\n",
    "\n",
    "# ---------------- Model Definitions ------------------\n",
    "\n",
    "# ResNet50 with CBAM and a more complex output structure\n",
    "def ResNet50_CBAM_Model():\n",
    "    input_shape = (224, 224, 3)\n",
    "    resnet50 = tf.keras.applications.ResNet50(include_top=False, input_shape=input_shape, weights='imagenet')\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(resnet50.output)\n",
    "    x = layers.Dense(units=1024, activation='relu')(x)\n",
    "    x = layers.Dense(units=512, activation='relu')(x)\n",
    "    x = layers.Dense(units=256, activation='relu')(x)\n",
    "    x = layers.Dense(units=128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "\n",
    "    x = layers.Reshape((1, 1, 128))(x)\n",
    "    x_max = layers.GlobalMaxPooling2D()(x)\n",
    "    x_avg = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Concatenate()([x_max, x_avg])\n",
    "    x = layers.Dense(units=4, activation='softmax')(x)\n",
    "\n",
    "    model = models.Model(inputs=resnet50.input, outputs=x, name='ResNet50_CBAM_Model')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf379f9f-b588-43f6-9c92-678c5bc6b5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet50_CBAM_Model()\n",
    "\n",
    "# Compile models\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6tFF6W3DWT9z",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6tFF6W3DWT9z",
    "outputId": "abc080d8-90c9-43dd-cfb7-044c037a9391"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Paths to the dataset\n",
    "train_dir = 'BrainTumor_MRI/Training'\n",
    "test_dir = 'BrainTumor_MRI/Testing'\n",
    "output_dir = 'NON_IID/'\n",
    "\n",
    "# Class names\n",
    "classes = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
    "\n",
    "# Number of samples to be taken for each model\n",
    "samples = {\n",
    "    'Model_1': {'glioma': (1021, 210), 'meningioma': (100, 32), 'notumor': (100, 50), 'pituitary': (100, 50)},\n",
    "    'Model_2': {'glioma': (100, 30), 'meningioma': (1039, 210), 'notumor': (100, 50), 'pituitary': (100, 50)},\n",
    "    'Model_3': {'glioma': (100, 30), 'meningioma': (100, 32), 'notumor': (1295, 255), 'pituitary': (100, 50)},\n",
    "    'Model_4': {'glioma': (100, 30), 'meningioma': (100, 32), 'notumor': (100, 50), 'pituitary': (1157, 150)}\n",
    "}\n",
    "\n",
    "# Function to copy files to the appropriate directories\n",
    "def copy_files(file_list, dest_dir):\n",
    "    for file_path in file_list:\n",
    "        shutil.copy(file_path, dest_dir)\n",
    "\n",
    "# Splitting the dataset\n",
    "for model, splits in samples.items():\n",
    "    model_dir = os.path.join(output_dir, model)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    for cls, (train_size, test_size) in splits.items():\n",
    "        # Train directory and files\n",
    "        train_class_dir = os.path.join(train_dir, cls)\n",
    "        train_files = [os.path.join(train_class_dir, f) for f in os.listdir(train_class_dir)]\n",
    "\n",
    "        # Test directory and files\n",
    "        test_class_dir = os.path.join(test_dir, cls)\n",
    "        test_files = [os.path.join(test_class_dir, f) for f in os.listdir(test_class_dir)]\n",
    "\n",
    "        # Split the train files for the model\n",
    "        train_subset, _ = train_test_split(train_files, train_size=train_size, random_state=42)\n",
    "\n",
    "        # Split the test files for the model\n",
    "        test_subset, _ = train_test_split(test_files, train_size=test_size, random_state=42)\n",
    "\n",
    "        # Copy train files\n",
    "        model_train_dir = os.path.join(model_dir, 'train', cls)\n",
    "        os.makedirs(model_train_dir, exist_ok=True)\n",
    "        copy_files(train_subset, model_train_dir)\n",
    "\n",
    "        # Copy test files\n",
    "        model_test_dir = os.path.join(model_dir, 'test', cls)\n",
    "        os.makedirs(model_test_dir, exist_ok=True)\n",
    "        copy_files(test_subset, model_test_dir)\n",
    "\n",
    "print(\"Dataset split completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t9OCZ_VMa22v",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t9OCZ_VMa22v",
    "outputId": "e2f1e360-4935-4859-8f89-6244aff8834d"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define paths for training and testing datasets\n",
    "source_train_dir = \"Federated_Learning_NON_IID/Model_1/train\"\n",
    "source_test_dir = \"Federated_Learning_NON_IID/Model_1/test\"\n",
    "\n",
    "# Define class names\n",
    "classes = [\"glioma\", \"meningioma\", \"notumor\", \"pituitary\"]\n",
    "\n",
    "# Check if images are in the correct source directory\n",
    "for class_name in classes:\n",
    "    class_train_folder = os.path.join(source_train_dir, class_name)\n",
    "    class_test_folder = os.path.join(source_test_dir, class_name)\n",
    "\n",
    "    if not os.path.exists(class_train_folder):\n",
    "        print(f\"Source training folder for {class_name} does not exist!\")\n",
    "    if not os.path.exists(class_test_folder):\n",
    "        print(f\"Source testing folder for {class_name} does not exist!\")\n",
    "    else:\n",
    "        print(f\"Source folders exist and contain {len(os.listdir(class_train_folder))} training images and {len(os.listdir(class_test_folder))} testing images for {class_name}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cgCedJnHdyj3",
   "metadata": {
    "id": "cgCedJnHdyj3"
   },
   "outputs": [],
   "source": [
    "def loss_classifier(predictions, labels):\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=predictions)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "\n",
    "def loss_dataset(model, dataset, loss_f):\n",
    "    loss = 0\n",
    "    for idx, (features, labels) in enumerate(dataset):\n",
    "        # Convert PyTorch tensors to NumPy and then to TensorFlow tensors\n",
    "        features_np = features.detach().cpu().numpy()\n",
    "        labels_np = labels.detach().cpu().numpy()\n",
    "\n",
    "        features_tf = tf.convert_to_tensor(features_np, dtype=tf.float32)\n",
    "        labels_tf = tf.convert_to_tensor(labels_np, dtype=tf.int32)\n",
    "\n",
    "        predictions = model(features_tf)\n",
    "        loss += loss_f(predictions, labels_tf)\n",
    "    loss /= (idx + 1)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def accuracy_dataset(model, dataset):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for features, labels in dataset:\n",
    "        # Convert PyTorch tensors to NumPy and then to TensorFlow tensors\n",
    "        features_np = features.detach().cpu().numpy()\n",
    "        labels_np = labels.detach().cpu().numpy()\n",
    "\n",
    "        features_tf = tf.convert_to_tensor(features_np, dtype=tf.float32)\n",
    "        labels_tf = tf.convert_to_tensor(labels_np, dtype=tf.int32)\n",
    "\n",
    "        predictions = model(features_tf)\n",
    "        predicted = tf.argmax(predictions, axis=1, output_type=tf.int32)\n",
    "        correct += tf.reduce_sum(tf.cast(tf.equal(predicted, labels_tf), tf.int32)).numpy()\n",
    "        total += labels_tf.shape[0]\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def train_step(model, model_0, mu, optimizer, train_data, loss_f):\n",
    "    total_loss = 0\n",
    "    for idx, (features, labels) in enumerate(train_data):\n",
    "        # Convert PyTorch tensors to NumPy and then to TensorFlow tensors\n",
    "        features_np = features.detach().cpu().numpy()\n",
    "        labels_np = labels.detach().cpu().numpy()\n",
    "\n",
    "        features_tf = tf.convert_to_tensor(features_np, dtype=tf.float32)\n",
    "        labels_tf = tf.convert_to_tensor(labels_np, dtype=tf.int32)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(features_tf)\n",
    "            loss = loss_f(predictions, labels_tf)\n",
    "            loss += mu / 2 * difference_models_norm_2(model, model_0)\n",
    "\n",
    "        total_loss += loss\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return total_loss / (idx + 1)\n",
    "\n",
    "\n",
    "def local_learning(model, mu, optimizer, train_data, epochs, loss_f):\n",
    "    # Clone the model instead of using deepcopy\n",
    "    model_0 = tf.keras.models.clone_model(model)\n",
    "    model_0.set_weights(model.get_weights())  # Set weights to be identical initially\n",
    "\n",
    "    for e in range(epochs):\n",
    "        local_loss = train_step(model, model_0, mu, optimizer, train_data, loss_f)\n",
    "\n",
    "    return local_loss\n",
    "\n",
    "\n",
    "def difference_models_norm_2(model_1, model_2):\n",
    "    norm = tf.reduce_sum([tf.reduce_sum(tf.square(w1 - w2)) for w1, w2 in zip(model_1.trainable_variables, model_2.trainable_variables)])\n",
    "    return norm\n",
    "\n",
    "def set_to_zero_model_weights(model):\n",
    "    for layer_weights in model.trainable_variables:\n",
    "        layer_weights.assign(tf.zeros_like(layer_weights))\n",
    "\n",
    "def average_models(model, clients_models_hist, weights):\n",
    "    set_to_zero_model_weights(model)\n",
    "    for k, client_hist in enumerate(clients_models_hist):\n",
    "        for idx, layer_weights in enumerate(model.trainable_variables):\n",
    "            contribution = client_hist[idx] * weights[k]\n",
    "            layer_weights.assign_add(contribution)\n",
    "\n",
    "#_______________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bPsjZ3UYd9b4",
   "metadata": {
    "id": "bPsjZ3UYd9b4"
   },
   "outputs": [],
   "source": [
    "def FedProx(model, training_sets, n_iter, testing_sets, mu=0, epochs=5, lr=0.01, decay=1):\n",
    "    # Verify that `model` is a Keras model instance\n",
    "    if not isinstance(model, tf.keras.Model):\n",
    "        raise TypeError(\"The provided model is not a TensorFlow Keras model. Please provide a valid Keras model.\")\n",
    "\n",
    "    loss_f = loss_classifier\n",
    "    K = len(training_sets)\n",
    "    n_samples = sum([len(db) for db in training_sets])\n",
    "    weights = [len(db) / n_samples for db in training_sets]\n",
    "    print(\"Clients' weights:\", weights)\n",
    "\n",
    "    # Initialize history lists for training and testing\n",
    "    train_loss_hist = []\n",
    "    train_acc_hist = []\n",
    "    test_loss_hist = []\n",
    "    test_acc_hist = []\n",
    "    models_hist = []\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        clients_params = []\n",
    "        clients_losses = []\n",
    "        clients_accuracies = []\n",
    "\n",
    "        for k in range(K):\n",
    "            # Clone the model and set weights for local training\n",
    "            local_model = tf.keras.models.clone_model(model)\n",
    "            local_model.set_weights(model.get_weights())\n",
    "\n",
    "            local_optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "\n",
    "            # Perform local training and track the loss\n",
    "            local_loss = local_learning(local_model, mu, local_optimizer, training_sets[k], epochs, loss_f)\n",
    "            clients_losses.append(local_loss)\n",
    "\n",
    "            # Track training accuracy for the client\n",
    "            train_acc = accuracy_dataset(local_model, training_sets[k])\n",
    "            clients_accuracies.append(train_acc)\n",
    "\n",
    "            # Store model parameters (deep copy to ensure immutability)\n",
    "            clients_params.append([tf.identity(tens_param) for tens_param in local_model.trainable_variables])\n",
    "\n",
    "        # Average the local models into the global model\n",
    "        average_models(model, clients_params, weights=weights)\n",
    "        models_hist.append(deepcopy(clients_params))\n",
    "\n",
    "        # Collect metrics for this iteration\n",
    "        train_loss_hist.append(clients_losses)\n",
    "        train_acc_hist.append(clients_accuracies)\n",
    "\n",
    "        # Compute testing metrics using the global model\n",
    "        test_loss_hist.append([loss_dataset(model, dl, loss_f).numpy() for dl in testing_sets])\n",
    "        test_acc_hist.append([accuracy_dataset(model, dl) for dl in testing_sets])\n",
    "\n",
    "        # Update learning rate by decay factor\n",
    "        lr *= decay\n",
    "        print(f'====> i: {i+1} Server Test Accuracy: {test_acc_hist[-1]}')\n",
    "\n",
    "    return model, train_loss_hist, train_acc_hist, test_loss_hist, test_acc_hist\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UhDohgk3gTpd",
   "metadata": {
    "id": "UhDohgk3gTpd"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_acc_loss(title: str, train_loss_hist: list, train_acc_hist: list,\n",
    "                  test_loss_hist: list, test_acc_hist: list):\n",
    "    plt.figure(figsize=(15, 5))  # Make the plot wider\n",
    "\n",
    "    # Plot training loss\n",
    "    plt.subplot(2, 2, 1)\n",
    "    lines = plt.plot(train_loss_hist)\n",
    "    plt.title(\"Training Loss\")\n",
    "    plt.legend(lines, [f\"C{i+1}\" for i in range(len(train_loss_hist[0]))])\n",
    "\n",
    "    # Plot training accuracy\n",
    "    plt.subplot(2, 2, 2)\n",
    "    lines = plt.plot(train_acc_hist)\n",
    "    plt.title(\"Training Accuracy\")\n",
    "    plt.legend(lines, [f\"C{i+1}\" for i in range(len(train_acc_hist[0]))])\n",
    "\n",
    "    # Plot testing loss\n",
    "    plt.subplot(2, 2, 3)\n",
    "    lines = plt.plot(test_loss_hist)\n",
    "    plt.title(\"Testing Loss\")\n",
    "    plt.legend(lines, [f\"C{i+1}\" for i in range(len(test_loss_hist[0]))])\n",
    "\n",
    "    # Plot testing accuracy\n",
    "    plt.subplot(2, 2, 4)\n",
    "    lines = plt.plot(test_acc_hist)\n",
    "    plt.title(\"Testing Accuracy\")\n",
    "    plt.legend(lines, [f\"C{i+1}\" for i in range(len(test_acc_hist[0]))])\n",
    "\n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def save_history_to_file(filename, n_iter, train_loss_hist, train_acc_hist,\n",
    "                         test_loss_hist, test_acc_hist):\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(\"FedProx Training Results\\n\")\n",
    "        f.write(\"=\"*50 + \"\\n\")\n",
    "        f.write(f\"Number of iterations: {n_iter}\\n\\n\")\n",
    "\n",
    "        # Write the history for each iteration\n",
    "        for i in range(n_iter):\n",
    "            f.write(f\"Iteration {i+1}:\\n\")\n",
    "            f.write(f\"Train Loss: {train_loss_hist[i]}\\n\")\n",
    "            f.write(f\"Train Accuracy: {train_acc_hist[i]}\\n\")\n",
    "            f.write(f\"Test Loss: {test_loss_hist[i]}\\n\")\n",
    "            f.write(f\"Test Accuracy: {test_acc_hist[i]}\\n\")\n",
    "            f.write(\"-\"*50 + \"\\n\")\n",
    "\n",
    "    print(f\"Training history saved to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wKhKga98r2Xa",
   "metadata": {
    "id": "wKhKga98r2Xa"
   },
   "source": [
    "# Data Agumentation\n",
    "###'Model 1': {'glioma': (1021, 210), 'meningioma': (100, 32), 'notumor': (100, 50), 'pituitary': (100, 50)},\n",
    "###'Model 2': {'glioma': (100, 30), 'meningioma': (1039, 210), 'notumor': (100, 50), 'pituitary': (100, 50)},\n",
    "###'Model 3': {'glioma': (100, 30), 'meningioma': (100, 32), 'notumor': (1295, 255), 'pituitary': (100, 50)},\n",
    "###'Model 4': {'glioma': (100, 30), 'meningioma': (100, 32), 'notumor': (100, 50), 'pituitary': (1157, 150)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "btaeN3tzkWvg",
   "metadata": {
    "id": "btaeN3tzkWvg"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define data augmentation strategy\n",
    "augmentation = ImageDataGenerator(\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    rotation_range=90,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    channel_shift_range=0.1, # extra\n",
    "    vertical_flip=True,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XqxlyMikka3K",
   "metadata": {
    "id": "XqxlyMikka3K"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img, array_to_img\n",
    "import numpy as np\n",
    "\n",
    "def augment_class_images(input_dir, target_dir, target_count, augmentation):\n",
    "    current_count = len(os.listdir(input_dir))\n",
    "    n_to_generate = target_count - current_count\n",
    "\n",
    "    # If current count is already sufficient, no augmentation is needed\n",
    "    if n_to_generate <= 0:\n",
    "        print(f\"No augmentation needed for {input_dir}\")\n",
    "        return\n",
    "\n",
    "    img_files = os.listdir(input_dir)\n",
    "    i = 0\n",
    "\n",
    "    while i < n_to_generate:\n",
    "        img_file = img_files[i % current_count]\n",
    "        img_path = os.path.join(input_dir, img_file)\n",
    "\n",
    "        # Load and augment image\n",
    "        image = load_img(img_path)\n",
    "        image_array = img_to_array(image)\n",
    "        image_array = np.expand_dims(image_array, axis=0)\n",
    "\n",
    "        # Generate augmented images\n",
    "        for batch in augmentation.flow(image_array, batch_size=1):\n",
    "            new_img = array_to_img(batch[0], scale=True)\n",
    "            new_img.save(os.path.join(target_dir, f\"aug_{i}.jpg\"))\n",
    "            i += 1\n",
    "            if i >= n_to_generate:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9HeNckyPs22a",
   "metadata": {
    "id": "9HeNckyPs22a"
   },
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006461f5-7d9f-4c18-b534-1f9424fc8edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "augment_class_images(\n",
    "    input_dir=\"Federated_Learning_NON_IID/Model_4/train/glioma\",\n",
    "    target_dir=\"Federated_Learning_NON_IID/Model_4/train/glioma\",\n",
    "    target_count=1157,\n",
    "    augmentation=augmentation\n",
    ")\n",
    "\n",
    "augment_class_images(\n",
    "    input_dir=\"Federated_Learning_NON_IID/Model_4/train/meningioma\",\n",
    "    target_dir=\"Federated_Learning_NON_IID/Model_4/train/meningioma\",\n",
    "    target_count=1157,\n",
    "    augmentation=augmentation\n",
    ")\n",
    "\n",
    "augment_class_images(\n",
    "    input_dir=\"Federated_Learning_NON_IID/Model_4/train/notumor\",\n",
    "    target_dir=\"Federated_Learning_NON_IID/Model_4/train/notumor\",\n",
    "    target_count=1157,\n",
    "    augmentation=augmentation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4rpKDNE9qIP3",
   "metadata": {
    "id": "4rpKDNE9qIP3"
   },
   "source": [
    "Testing data agumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f4055c-9c11-4684-9ddd-6d8f535bc19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "augment_class_images(\n",
    "    input_dir=\"Federated_Learning_NON_IID/Model_4/test/glioma\",\n",
    "    target_dir=\"Federated_Learning_NON_IID/Model_4/test/glioma\",\n",
    "    target_count=150,\n",
    "    augmentation=augmentation\n",
    ")\n",
    "\n",
    "augment_class_images(\n",
    "    input_dir=\"Federated_Learning_NON_IID/Model_4/test/meningioma\",\n",
    "    target_dir=\"Federated_Learning_NON_IID/Model_4/test/meningioma\",\n",
    "    target_count=150,\n",
    "    augmentation=augmentation\n",
    ")\n",
    "\n",
    "augment_class_images(\n",
    "    input_dir=\"Federated_Learning_NON_IID/Model_4/test/notumor\",\n",
    "    target_dir=\"Federated_Learning_NON_IID/Model_4/test/notumor\",\n",
    "    target_count=150,\n",
    "    augmentation=augmentation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8TAY73LsbaJj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8TAY73LsbaJj",
    "outputId": "b3df73f4-be4f-4ac8-b4f4-76188649286a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Directories for each client\n",
    "split_dirs = [\n",
    "    \"Federated_Learning_NON_IID/Model_1\",\n",
    "    \"Federated_Learning_NON_IID/Model_2\",\n",
    "    \"Federated_Learning_NON_IID/Model_3\",\n",
    "    \"Federated_Learning_NON_IID/Model_4\"\n",
    "]\n",
    "\n",
    "# Number of clients\n",
    "n_clients = len(split_dirs)\n",
    "\n",
    "# Updated image transformations: Normalize first, then permute\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
    "    transforms.ToTensor(),          # Convert image to tensor format (C, H, W)\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize\n",
    "    transforms.Lambda(lambda x: x.permute(1, 2, 0))  # Permute to (H, W, C)\n",
    "])\n",
    "\n",
    "def get_tumor_dataloaders(split_dirs, batch_size=25, shuffle=True):\n",
    "    \"\"\"\n",
    "    Returns data loaders for all clients for both training and testing sets.\n",
    "    \"\"\"\n",
    "    tumor_iid_train_dls = []\n",
    "    tumor_iid_test_dls = []\n",
    "\n",
    "    for client_idx, client_dir in enumerate(split_dirs):\n",
    "        # Get the directory for the current client\n",
    "        train_dir = os.path.join(client_dir, 'train')\n",
    "        test_dir = os.path.join(client_dir, 'test')\n",
    "\n",
    "        # Check if the directories exist\n",
    "        if not os.path.exists(train_dir) or not os.path.exists(test_dir):\n",
    "            print(f\"Directory not found for client {client_idx + 1}:\")\n",
    "            print(f\"Train dir: {train_dir}\")\n",
    "            print(f\"Test dir: {test_dir}\")\n",
    "            continue\n",
    "\n",
    "        # Load training data for the current client\n",
    "        train_dataset = datasets.ImageFolder(train_dir, transform=transform)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "        # Load testing data for the current client\n",
    "        test_dataset = datasets.ImageFolder(test_dir, transform=transform)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # Append the dataloaders for the current client to the list\n",
    "        tumor_iid_train_dls.append(train_loader)\n",
    "        tumor_iid_test_dls.append(test_loader)\n",
    "\n",
    "    return tumor_iid_train_dls, tumor_iid_test_dls\n",
    "\n",
    "# Get the training and testing data loaders\n",
    "Tumor_iid_train_dls, Tumor_iid_test_dls = get_tumor_dataloaders(split_dirs, batch_size=25)\n",
    "\n",
    "# Checking the sizes of the images in the data loaders to verify the shape\n",
    "for batch_idx, (images, labels) in enumerate(Tumor_iid_train_dls[0]):  # Checking for client 1\n",
    "    print(f\"Batch {batch_idx} image sizes: {images.size()}\")  # Should print torch.Size([25, 224, 224, 3])\n",
    "    break  # Check only the first batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yxiprF9weH9L",
   "metadata": {
    "id": "yxiprF9weH9L"
   },
   "outputs": [],
   "source": [
    "n_iter = 21\n",
    "# Execute FedProx with the new 4-client setup\n",
    "model, train_loss, train_acc, test_loss, test_acc = FedProx(\n",
    "    model, Tumor_iid_train_dls, n_iter, Tumor_iid_test_dls, mu=0.3, epochs=1, lr=0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979a7d6c-c046-49cb-81e6-8d6ed1990c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 20\n",
    "# Execute FedProx with the new 4-client setup\n",
    "model_1, train_loss_1, train_acc_1, test_loss_1, test_acc_1 = FedProx(\n",
    "    model, Tumor_iid_train_dls, n_iter, Tumor_iid_test_dls, mu=0.4, epochs=1, lr=0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11613a39-efa2-426e-8d28-bd3a242c21a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File to save the results\n",
    "output_file = \"results-mu=0.4.txt\"\n",
    "# Save the history to a file\n",
    "save_history_to_file(output_file, n_iter, train_loss_1, train_acc_1, test_loss_1, test_acc_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edf3bfe-0961-4ee8-a6e7-701e94ffb3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and testing metrics\n",
    "plot_acc_loss(\"FedProx Training and Testing Metrics for mu=0.4\",\n",
    "              train_loss_1, train_acc_1, test_loss_1, test_acc_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e14f97-c106-400b-a8f1-945672a562a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, classification_report\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
    "    transforms.ToTensor(),          # Convert image to tensor format (C, H, W)\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize\n",
    "    transforms.Lambda(lambda x: x.permute(1, 2, 0))  # Permute to (H, W, C)\n",
    "])\n",
    "\n",
    "# Load the testing dataset\n",
    "test_dir = 'BrainTumor_MRI/Testing'\n",
    "test_dataset = datasets.ImageFolder(test_dir, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Class names mapping (adjust if necessary)\n",
    "class_names = test_dataset.classes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c46c70-74c7-48e3-8b24-d676df2845e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the global model on the testing dataset\n",
    "def evaluate_global_model(model, test_loader):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_loss = 0\n",
    "\n",
    "    for features, labels in test_loader:\n",
    "        # Convert to TensorFlow tensors\n",
    "        features_np = features.detach().cpu().numpy()\n",
    "        labels_np = labels.detach().cpu().numpy()\n",
    "\n",
    "        features_tf = tf.convert_to_tensor(features_np, dtype=tf.float32)\n",
    "        labels_tf = tf.convert_to_tensor(labels_np, dtype=tf.int32)\n",
    "\n",
    "        # Get predictions\n",
    "        predictions = model(features_tf)\n",
    "        predicted = tf.argmax(predictions, axis=1, output_type=tf.int32)\n",
    "\n",
    "        # Accumulate predictions and labels\n",
    "        all_preds.extend(predicted.numpy())\n",
    "        all_labels.extend(labels_np)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels_tf, logits=predictions)\n",
    "        total_loss += tf.reduce_mean(loss).numpy()\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    print(f\"\\nConfusion Matrix:\\n{conf_matrix}\")\n",
    "    print(f\"\\nAccuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Loss: {total_loss / len(test_loader):.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "\n",
    "    # Print detailed classification report\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(all_labels, all_preds, target_names=class_names))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f60dbf-f160-4821-ab56-1517446eb841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the testing dataset\n",
    "evaluate_global_model(model_1, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70b27b4-99cf-4c32-8e62-ab780ccec2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define the path to save the Keras model\n",
    "MODEL_PATH = 'global_model_keras/'\n",
    "\n",
    "# Save the trained global model in the Keras HDF5 or SavedModel format\n",
    "model_1.save(MODEL_PATH)  # This ensures the model includes Keras metadata\n",
    "\n",
    "print(f\"Global model saved successfully at: {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95bb11a-ca2f-4257-b994-39355b069e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
    "import numpy as np\n",
    "\n",
    "# Load the saved Keras model\n",
    "MODEL_PATH = 'global_model_keras/'\n",
    "model = tf.keras.models.load_model(MODEL_PATH)\n",
    "\n",
    "# Define class names\n",
    "class_names = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
    "\n",
    "# Function to predict the class of a single image\n",
    "def predict_image(image_path, model):\n",
    "    # Load and preprocess the image\n",
    "    image = load_img(image_path, target_size=(224, 224))  # Resize to 224x224\n",
    "    image_array = img_to_array(image)  # Convert to numpy array\n",
    "    image_array = np.expand_dims(image_array, axis=0)  # Add batch dimension\n",
    "    image_array = preprocess_input(image_array)  # Normalize for VGG19\n",
    "\n",
    "    # Perform inference\n",
    "    predictions = model(image_array, training=False)  # Inference mode\n",
    "    probabilities = tf.nn.softmax(predictions[0]).numpy()  # Apply softmax\n",
    "    predicted_class = np.argmax(probabilities)\n",
    "\n",
    "    # Print the predicted class and probabilities\n",
    "    print(f\"Predicted Class: {class_names[predicted_class]}\")\n",
    "    print(f\"Probabilities: {probabilities}\")\n",
    "\n",
    "# Test the model with a single image\n",
    "image_path = 'BrainTumor_MRI/M.jpg'\n",
    "predict_image(image_path, model_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85021422-e203-4908-8faf-c5fe6ed2d889",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'BrainTumor_MRI/P.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96864f89-3cf9-4d2f-885e-e1690c5bfe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(image_path, model_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b631d62b-8852-47e7-8c2f-b1e209eb6a00",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b0e4fc-6b13-43c6-9c47-01606dc18225",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = 'global_model_keras/'\n",
    "model = tf.keras.models.load_model(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359980fc-34ce-493e-9698-32222582ed78",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 30\n",
    "# Execute FedProx with the new 4-client setup\n",
    "model_f, train_loss_f, train_acc_f, test_loss_f, test_acc_f = FedProx(\n",
    "    model, Tumor_iid_train_dls, n_iter, Tumor_iid_test_dls, mu=0.4, epochs=1, lr=0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899797c5-5548-44b0-bf76-bedae7d27365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# File to save the results\n",
    "output_file = \"results-mu=0.4_30-rounds.txt\"\n",
    "\n",
    "# Initial parameters\n",
    "MODEL_PATH = 'global_model_keras/global_model_rounds_10.h5'\n",
    "n_iter = 30\n",
    "rounds_per_segment = 5\n",
    "mu = 0.4\n",
    "epochs = 1\n",
    "lr = 0.001\n",
    "\n",
    "# Initialize lists to store cumulative training and testing metrics\n",
    "all_train_loss = []\n",
    "all_train_acc = []\n",
    "all_test_loss = []\n",
    "all_test_acc = []\n",
    "\n",
    "# Load the initial model\n",
    "model = tf.keras.models.load_model(MODEL_PATH)\n",
    "\n",
    "# Function for FedProx training in segments\n",
    "for i in range(0, n_iter, rounds_per_segment):\n",
    "    print(f\"Starting rounds {i+1} to {i+rounds_per_segment}\")\n",
    "    \n",
    "    # Train for the specified number of rounds in this segment\n",
    "    model, train_loss_f, train_acc_f, test_loss_f, test_acc_f = FedProx(\n",
    "        model, Tumor_iid_train_dls, rounds_per_segment, Tumor_iid_test_dls, mu=mu, epochs=epochs, lr=lr\n",
    "    )\n",
    "    \n",
    "    # Append the metrics to the cumulative lists\n",
    "    all_train_loss.extend(train_loss_f)\n",
    "    all_train_acc.extend(train_acc_f)\n",
    "    all_test_loss.extend(test_loss_f)\n",
    "    all_test_acc.extend(test_acc_f)\n",
    "    \n",
    "    # Save the model at the end of each segment\n",
    "    segment_model_path = f'{MODEL_PATH}global_model_rounds_{i+rounds_per_segment}.h5'\n",
    "    model.save(segment_model_path)\n",
    "    print(f\"Model saved at: {segment_model_path}\")\n",
    "\n",
    "    # Reload the model to ensure continuity for the next segment\n",
    "    model = tf.keras.models.load_model(segment_model_path)\n",
    "\n",
    "# Save the cumulative history to a file after all rounds are complete\n",
    "save_history_to_file(output_file, n_iter, all_train_loss, all_train_acc, all_test_loss, all_test_acc)\n",
    "\n",
    "# Plot the cumulative training and testing metrics\n",
    "plot_acc_loss(\"FedProx Training and Testing Metrics for mu=0.4, 30 rounds\",\n",
    "              all_train_loss, all_train_acc, all_test_loss, all_test_acc)\n",
    "\n",
    "print(\"Training complete and results saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b9af92-6e99-424e-953c-af3ca211e480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# File to save the results\n",
    "output_file = \"results-mu=0.4_30-rounds.txt\"\n",
    "\n",
    "# Initial parameters\n",
    "MODEL_PATH = 'global_model_keras/global_model_rounds_15.h5'\n",
    "n_iter = 30\n",
    "rounds_per_segment = 5\n",
    "mu = 0.4\n",
    "epochs = 1\n",
    "lr = 0.001\n",
    "\n",
    "# Initialize lists to store cumulative training and testing metrics\n",
    "all_train_loss = []\n",
    "all_train_acc = []\n",
    "all_test_loss = []\n",
    "all_test_acc = []\n",
    "\n",
    "# Load the initial model\n",
    "model = tf.keras.models.load_model(MODEL_PATH)\n",
    "\n",
    "# Function for FedProx training in segments\n",
    "for i in range(15, n_iter, rounds_per_segment):\n",
    "    print(f\"Starting rounds {i+1} to {i+rounds_per_segment}\")\n",
    "    \n",
    "    # Train for the specified number of rounds in this segment\n",
    "    model, train_loss_f, train_acc_f, test_loss_f, test_acc_f = FedProx(\n",
    "        model, Tumor_iid_train_dls, rounds_per_segment, Tumor_iid_test_dls, mu=mu, epochs=epochs, lr=lr\n",
    "    )\n",
    "    \n",
    "    # Append the metrics to the cumulative lists\n",
    "    all_train_loss.extend(train_loss_f)\n",
    "    all_train_acc.extend(train_acc_f)\n",
    "    all_test_loss.extend(test_loss_f)\n",
    "    all_test_acc.extend(test_acc_f)\n",
    "    \n",
    "    # Save the model at the end of each segment\n",
    "    segment_model_path = f'{MODEL_PATH}global_model_rounds_{i+rounds_per_segment}.h5'\n",
    "    model.save(segment_model_path)\n",
    "    print(f\"Model saved at: {segment_model_path}\")\n",
    "\n",
    "    # Reload the model to ensure continuity for the next segment\n",
    "    model = tf.keras.models.load_model(segment_model_path)\n",
    "\n",
    "# Save the cumulative history to a file after all rounds are complete\n",
    "save_history_to_file(output_file, n_iter, all_train_loss, all_train_acc, all_test_loss, all_test_acc)\n",
    "\n",
    "# Plot the cumulative training and testing metrics\n",
    "plot_acc_loss(\"FedProx Training and Testing Metrics for mu=0.4, 30 rounds\",\n",
    "              all_train_loss, all_train_acc, all_test_loss, all_test_acc)\n",
    "\n",
    "print(\"Training complete and results saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df574044-533a-4c4c-96a4-d35a4f300e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cumulative history to a file after all rounds are complete\n",
    "# Only use data for the last 15 rounds\n",
    "last_15_train_loss = all_train_loss[-15:]  # Get last 15 entries\n",
    "last_15_train_acc = all_train_acc[-15:]\n",
    "last_15_test_loss = all_test_loss[-15:]\n",
    "last_15_test_acc = all_test_acc[-15:]\n",
    "\n",
    "# Save the last 15 rounds of history to a file\n",
    "save_history_to_file(output_file, 15, last_15_train_loss, last_15_train_acc, last_15_test_loss, last_15_test_acc)\n",
    "\n",
    "# Plot the training and testing metrics for the last 15 rounds\n",
    "plot_acc_loss(\"FedProx Training and Testing Metrics (Last 15 Rounds, mu=0.4)\", \n",
    "              last_15_train_loss, last_15_train_acc, last_15_test_loss, last_15_test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc24e45c-e0eb-4e98-be0b-40d704276390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data for the last 15 rounds\n",
    "last_15_train_loss = all_train_loss[-15:]  # Get last 15 entries\n",
    "last_15_train_acc = all_train_acc[-15:]\n",
    "last_15_test_loss = all_test_loss[-15:]\n",
    "last_15_test_acc = all_test_acc[-15:]\n",
    "\n",
    "# Define the output file path\n",
    "output_file = \"last_15_rounds_history.txt\"\n",
    "\n",
    "# Save the last 15 rounds of history to a file\n",
    "def save_last_15_history_to_file(filename, train_loss, train_acc, test_loss, test_acc):\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(\"FedProx Training and Testing Metrics (Last 15 Rounds)\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\")\n",
    "        \n",
    "        for i in range(len(train_loss)):\n",
    "            try:\n",
    "                # Handle cases where elements may be lists\n",
    "                train_loss_val = float(train_loss[i][0]) if isinstance(train_loss[i], list) else float(train_loss[i])\n",
    "                train_acc_val = float(train_acc[i][0]) if isinstance(train_acc[i], list) else float(train_acc[i])\n",
    "                test_loss_val = float(test_loss[i][0]) if isinstance(test_loss[i], list) else float(test_loss[i])\n",
    "                test_acc_val = float(test_acc[i][0]) if isinstance(test_acc[i], list) else float(test_acc[i])\n",
    "                \n",
    "                f.write(f\"Round {i + 16}:\\n\")  # Adjust round numbers to 16-30\n",
    "                f.write(f\"Train Loss: {train_loss_val:.4f}\\n\")\n",
    "                f.write(f\"Train Accuracy: {train_acc_val:.4f}\\n\")\n",
    "                f.write(f\"Test Loss: {test_loss_val:.4f}\\n\")\n",
    "                f.write(f\"Test Accuracy: {test_acc_val:.4f}\\n\")\n",
    "                f.write(\"-\" * 50 + \"\\n\")\n",
    "                \n",
    "            except (ValueError, TypeError) as e:\n",
    "                f.write(f\"Error processing round {i + 16}: {e}\\n\")\n",
    "                f.write(\"-\" * 50 + \"\\n\")\n",
    "    \n",
    "    print(f\"History for the last 15 rounds saved to {filename}\")\n",
    "\n",
    "# Save the data to the file\n",
    "save_last_15_history_to_file(output_file, last_15_train_loss, last_15_train_acc, last_15_test_loss, last_15_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c88a68a-fad8-4ab4-984f-a828eb354dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, classification_report\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
    "    transforms.ToTensor(),          # Convert image to tensor format (C, H, W)\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize\n",
    "    transforms.Lambda(lambda x: x.permute(1, 2, 0))  # Permute to (H, W, C)\n",
    "])\n",
    "\n",
    "# Load the testing dataset\n",
    "test_dir = 'BrainTumor_MRI/Testing'\n",
    "test_dataset = datasets.ImageFolder(test_dir, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Class names mapping (adjust if necessary)\n",
    "class_names = test_dataset.classes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e608438f-d5af-4229-a060-1823efa41f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class names\n",
    "class_names = [\"glioma\", \"meningioma\", \"notumor\", \"pituitary\"]  # Modify according to your classes\n",
    "\n",
    "# Function to evaluate the global model on the testing dataset\n",
    "def evaluate_global_model(model, test_loader):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_loss = 0\n",
    "\n",
    "    for features, labels in test_loader:\n",
    "        # Convert to TensorFlow tensors\n",
    "        features_np = features.detach().cpu().numpy()\n",
    "        labels_np = labels.detach().cpu().numpy()\n",
    "\n",
    "        features_tf = tf.convert_to_tensor(features_np, dtype=tf.float32)\n",
    "        labels_tf = tf.convert_to_tensor(labels_np, dtype=tf.int32)\n",
    "\n",
    "        # Get predictions\n",
    "        predictions = model(features_tf)\n",
    "        predicted = tf.argmax(predictions, axis=1, output_type=tf.int32)\n",
    "\n",
    "        # Accumulate predictions and labels\n",
    "        all_preds.extend(predicted.numpy())\n",
    "        all_labels.extend(labels_np)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels_tf, logits=predictions)\n",
    "        total_loss += tf.reduce_mean(loss).numpy()\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    # Print detailed classification report\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(all_labels, all_preds, target_names=class_names))\n",
    "    print(f\"\\nAccuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Loss: {total_loss / len(test_loader):.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "\n",
    "    # Plot confusion matrix using Seaborn\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted3 Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b5c708-6fe0-4be7-b8b5-7968dc8ba177",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = 'global_model_keras/global_model_rounds_30.h5'\n",
    "model = tf.keras.models.load_model(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f003fa14-afdb-44a0-b9d0-0a0dd1bd8b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the testing dataset\n",
    "evaluate_global_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5ee6af-7bc6-47f9-9b55-398ac7ef3d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = 'global_model_keras/global_model_rounds_25.h5'\n",
    "model_a = tf.keras.models.load_model(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3763292b-c96d-460d-98cc-1e29590c2b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_global_model(model_a, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f0c05d-cfff-4ad4-9f9e-54ee45d4f163",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07e4b4e-d930-4253-a5d8-752db24cf27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
    "import numpy as np\n",
    "\n",
    "# Define class names\n",
    "class_names = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
    "\n",
    "# Function to predict the class of a single image\n",
    "def predict_image(image_path, model):\n",
    "    # Load and preprocess the image\n",
    "    image = load_img(image_path, target_size=(224, 224))  # Resize to 224x224\n",
    "    image_array = img_to_array(image)  # Convert to numpy array\n",
    "    image_array = np.expand_dims(image_array, axis=0)  # Add batch dimension\n",
    "    image_array = preprocess_input(image_array)  # Normalize for VGG19\n",
    "\n",
    "    # Perform inference\n",
    "    predictions = model(image_array, training=False)  # Inference mode\n",
    "    probabilities = tf.nn.softmax(predictions[0]).numpy()  # Apply softmax\n",
    "    predicted_class = np.argmax(probabilities)\n",
    "\n",
    "    # Print the predicted class and probabilities\n",
    "    print(f\"Predicted Class: {class_names[predicted_class]}\")\n",
    "    print(f\"Probabilities: {probabilities}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48422386-fdc3-487f-bd90-99cfecfae16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model with a single image\n",
    "image_path = 'BrainTumor_MRI/M.jpg'\n",
    "predict_image(image_path, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b691e720-ebb3-4cef-9e12-2e608f848318",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'BrainTumor_MRI/Testing/pituitary/Te-pi_0024.jpg'\n",
    "predict_image(image_path, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046de0eb-65e6-4b97-8b23-262cf8a07563",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'BrainTumor_MRI/Testing/pituitary/Te-pi_0024.jpg'\n",
    "predict_image(image_path, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a015c98-730c-4ca2-8398-997803bdc342",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'BrainTumor_MRI/Testing/meningioma/Te-me_0073.jpg'\n",
    "predict_image(image_path, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5854e0-320f-4c11-a6bf-7c6257ce5212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a range of mu values to search\n",
    "n_iter = 10\n",
    "mu_values = [0.9, 1.0]\n",
    "best_mu = None\n",
    "best_test_loss = float('inf')  # Set to inf to minimize loss\n",
    "best_test_acc = 0.0  # Set to 0 if you want to maximize accuracy\n",
    "\n",
    "# Grid search over mu values\n",
    "for mu in mu_values:\n",
    "    # Execute FedProx with the current mu value\n",
    "    model_test, train_loss, train_acc, test_loss, test_acc = FedProx(\n",
    "        model, Tumor_iid_train_dls, n_iter, Tumor_iid_test_dls, mu=mu, epochs=1, lr=0.001\n",
    "    )\n",
    "    \n",
    "    # Update best_mu if current mu yields better test performance\n",
    "    if test_loss < best_test_loss:  # Adjust if optimizing accuracy\n",
    "        best_mu = mu\n",
    "        best_test_loss = test_loss\n",
    "        best_test_acc = test_acc\n",
    "\n",
    "print(f\"Best mu: {best_mu} with Test Loss: {best_test_loss} and Test Accuracy: {best_test_acc}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
