{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fc1c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- Import Libraries -----------------\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, precision_score, recall_score, classification_report\n",
    "\n",
    "import seaborn as sns\n",
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e28747-b406-4a16-b735-c7c1512e4171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- ResNet50_Model Definition -----------------\n",
    "def ResNet50_Model():\n",
    "    input_shape = (224, 224, 3)\n",
    "    resnet50 = tf.keras.applications.ResNet50(include_top=False, input_shape=input_shape, weights='imagenet')\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(units=1024, activation='relu')(x)\n",
    "    x = layers.Dense(units=512, activation='relu')(x)\n",
    "    x = layers.Dense(units=256, activation='relu')(x)\n",
    "    x = layers.Dense(units=128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "\n",
    "    x = layers.Dense(units=4, activation='softmax')(x)\n",
    "\n",
    "    model = models.Model(inputs=resnet50.input, outputs=x, name='ResNet50_Model')\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24989fab-abf2-451a-891a-e5af072bf41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- Compile and Train -----------------\n",
    "def compile_and_train(model, train_data, test_data, epochs=10, learning_rate=0.001):\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(\n",
    "        train_data,\n",
    "        validation_data=test_data,\n",
    "        epochs=epochs,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab2a991-e6cc-41fe-8fff-09731233d144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- Evaluate model performance -----------------\n",
    "def evaluate_global_model(model, test_loader, client_idx, output_dir=\"Reset50_model_results\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Ensure the output directory exists\n",
    "    output_file = os.path.join(output_dir, f\"Model_Results_Client_{client_idx + 1}.txt\")\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_loss = 0\n",
    "\n",
    "    for features, labels in test_loader:\n",
    "        features_tf = tf.convert_to_tensor(features, dtype=tf.float32)\n",
    "\n",
    "        # Convert one-hot encoded labels to class indices\n",
    "        if len(labels.shape) > 1 and labels.shape[-1] > 1:  # Likely one-hot\n",
    "            labels_tf = tf.argmax(labels, axis=1, output_type=tf.int32)\n",
    "        else:\n",
    "            labels_tf = tf.convert_to_tensor(labels, dtype=tf.int32)\n",
    "\n",
    "        # Debugging: Print shapes\n",
    "        print(f\"Features shape: {features_tf.shape}, Labels shape: {labels_tf.shape}\")\n",
    "\n",
    "        # Get predictions\n",
    "        predictions = model(features_tf, training=False)\n",
    "        predicted = tf.argmax(predictions, axis=1, output_type=tf.int32)\n",
    "\n",
    "        # Accumulate predictions and labels\n",
    "        all_preds.extend(predicted.numpy())\n",
    "        all_labels.extend(labels_tf.numpy())\n",
    "\n",
    "\n",
    "        # Compute loss\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels_tf, logits=predictions)\n",
    "        total_loss += tf.reduce_mean(loss).numpy()\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    # Classification report\n",
    "    class_names = [\"glioma\", \"meningioma\", \"notumor\", \"pituitary\"] \n",
    "    classification_report_str = classification_report(all_labels, all_preds, target_names=class_names)\n",
    "\n",
    "    # Append metrics to a file\n",
    "    with open(output_file, \"w\") as file:\n",
    "        file.write(f\"Evaluation Metrics for Client {client_idx + 1}:\\n\")\n",
    "        file.write(f\"Accuracy: {accuracy * 100:.2f}%\\n\")\n",
    "        file.write(f\"Loss: {total_loss / len(test_loader):.4f}\\n\")\n",
    "        file.write(f\"Precision: {precision:.4f}\\n\")\n",
    "        file.write(f\"Recall: {recall:.4f}\\n\")\n",
    "        file.write(f\"F1 Score: {f1:.4f}\\n\\n\")\n",
    "        file.write(\"Classification Report:\\n\")\n",
    "        file.write(classification_report_str)\n",
    "        file.write(\"\\nConfusion Matrix:\\n\")\n",
    "        file.write(\"\\n\".join([\"\\t\".join(map(str, row)) for row in conf_matrix]))\n",
    "        file.write(\"\\n\" + \"-\" * 50 + \"\\n\")\n",
    "\n",
    "    # Print metrics\n",
    "    print(\"\\nClassification Report:\\n\", classification_report_str)\n",
    "    print(f\"\\nAccuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Loss: {total_loss / len(test_loader):.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # Plot and save heatmap\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title(f'Confusion Matrix (Client {client_idx + 1})')\n",
    "    plt.savefig(os.path.join(output_dir, f\"confusion_matrix_Client_{client_idx + 1}.png\"))\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd1c44c",
   "metadata": {},
   "source": [
    "### Replace path with the respective NON-IID dataset paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3215e56-6f72-401c-a2e6-fc8235507557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories for each client\n",
    "\n",
    "split_dirs = [\n",
    "    \"../NON_IID/Model_1\",\n",
    "    \"../NON_IID/Model_2\",\n",
    "    \"../NON_IID/Model_3\",\n",
    "    \"../NON_IID/Model_4\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05a0a17",
   "metadata": {},
   "source": [
    "### Load the NON-IID dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "554cb072-483e-4efb-a73e-96f9efdd0ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 image sizes: torch.Size([25, 224, 224, 3])\n"
     ]
    }
   ],
   "source": [
    "# Updated image transformations: Normalize first, then permute\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
    "    transforms.ToTensor(),          # Convert image to tensor format (C, H, W)\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize\n",
    "    transforms.Lambda(lambda x: x.permute(1, 2, 0))  # Permute to (H, W, C)\n",
    "])\n",
    "\n",
    "def get_tumor_dataloaders(split_dirs, batch_size=25, shuffle=True):\n",
    "    \"\"\"\n",
    "    Returns data loaders for all clients for both training and testing sets.\n",
    "    \"\"\"\n",
    "    tumor_iid_train_dls = []\n",
    "    tumor_iid_test_dls = []\n",
    "\n",
    "    for client_idx, client_dir in enumerate(split_dirs):\n",
    "        # Get the directory for the current client\n",
    "        train_dir = os.path.join(client_dir, 'train')\n",
    "        test_dir = os.path.join(client_dir, 'test')\n",
    "\n",
    "        # Check if the directories exist\n",
    "        if not os.path.exists(train_dir) or not os.path.exists(test_dir):\n",
    "            print(f\"Directory not found for client {client_idx + 1}:\")\n",
    "            print(f\"Train dir: {train_dir}\")\n",
    "            print(f\"Test dir: {test_dir}\")\n",
    "            continue\n",
    "\n",
    "        # Load training data for the current client\n",
    "        train_dataset = datasets.ImageFolder(train_dir, transform=transform)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "        # Load testing data for the current client\n",
    "        test_dataset = datasets.ImageFolder(test_dir, transform=transform)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # Append the dataloaders for the current client to the list\n",
    "        tumor_iid_train_dls.append(train_loader)\n",
    "        tumor_iid_test_dls.append(test_loader)\n",
    "\n",
    "    return tumor_iid_train_dls, tumor_iid_test_dls\n",
    "\n",
    "# Get the training and testing data loaders\n",
    "Tumor_iid_train_dls, Tumor_iid_test_dls = get_tumor_dataloaders(split_dirs, batch_size=25)\n",
    "\n",
    "# Checking the sizes of the images in the data loaders to verify the shape\n",
    "for batch_idx, (images, labels) in enumerate(Tumor_iid_train_dls[0]):  # Checking for client 1\n",
    "    print(f\"Batch {batch_idx} image sizes: {images.size()}\")  # Should print torch.Size([25, 224, 224, 3])\n",
    "    break  # Check only the first batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c63034e",
   "metadata": {},
   "source": [
    "### Converts a PyTorch DataLoader to a TensorFlow dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e653b82-6b74-4ec2-b594-97c814448325",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def pytorch_to_tf_dataset(pytorch_loader, num_classes):\n",
    "    images, labels = [], []\n",
    "    for batch_images, batch_labels in pytorch_loader:\n",
    "        # Convert PyTorch tensors to NumPy arrays and permute to (Batch, Height, Width, Channels)\n",
    "        images.append(batch_images.permute(0, 2, 1, 3).numpy())\n",
    "        # One-hot encode labels\n",
    "        labels.append(to_categorical(batch_labels.numpy(), num_classes=num_classes))\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    images = np.concatenate(images, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "    \n",
    "    # Create TensorFlow dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "    return dataset.batch(pytorch_loader.batch_size)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7b0ffc",
   "metadata": {},
   "source": [
    "### Main method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1574570",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Iterate over all clients' datasets, here 4 clients, 4 datasets\n",
    "    for client_idx in range(len(split_dirs)):\n",
    "        print(f\"\\nTraining and testing on dataset: Model_{client_idx + 1}\\n\")\n",
    "        \n",
    "        # Load data for the current client\n",
    "        train_loader = Tumor_iid_train_dls[client_idx]\n",
    "        test_loader = Tumor_iid_test_dls[client_idx]\n",
    "\n",
    "        # Number of classes in your dataset\n",
    "        num_classes = 4\n",
    "\n",
    "        # Convert PyTorch DataLoaders to TensorFlow datasets\n",
    "        tf_train_dataset = pytorch_to_tf_dataset(train_loader, num_classes)\n",
    "        tf_test_dataset = pytorch_to_tf_dataset(test_loader, num_classes)\n",
    "\n",
    "        # Build Model\n",
    "        model = ResNet50_Model()\n",
    "\n",
    "        # Use these datasets for training\n",
    "        history = compile_and_train(model, tf_train_dataset, tf_test_dataset, epochs=10, learning_rate=0.0001)\n",
    "\n",
    "        # Evaluate and Save Results\n",
    "        evaluate_global_model(model, tf_test_dataset, client_idx, output_dir=\"Reset50_model_results\")\n",
    "\n",
    "        print(f\"Training and evaluation completed for Model_{client_idx + 1}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
