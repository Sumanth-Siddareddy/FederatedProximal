{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994d91bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- Import Libraries -----------------\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, precision_score, recall_score, classification_report\n",
    "\n",
    "import seaborn as sns\n",
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0948520-c5e4-47ee-8577-c3ff0af557ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- CBAM Block Definition -----------------\n",
    "def cbam_block(inputs, reduction_ratio=0.5):\n",
    "    channels = inputs.shape[-1]\n",
    "\n",
    "    # Channel Attention\n",
    "    avg_pool = layers.GlobalAveragePooling2D()(inputs)\n",
    "    max_pool = layers.GlobalMaxPooling2D()(inputs)\n",
    "    shared_layer_1 = layers.Dense(int(channels * reduction_ratio), activation='relu', use_bias=True)\n",
    "    shared_layer_2 = layers.Dense(channels, activation='relu', use_bias=True)\n",
    "\n",
    "    avg_pool = shared_layer_1(avg_pool)\n",
    "    avg_pool = shared_layer_2(avg_pool)\n",
    "    max_pool = shared_layer_1(max_pool)\n",
    "    max_pool = shared_layer_2(max_pool)\n",
    "\n",
    "    attention = layers.Add()([avg_pool, max_pool])\n",
    "    attention = layers.Activation('sigmoid')(attention)\n",
    "    attention = layers.Reshape((1, 1, channels))(attention)\n",
    "    scaled_inputs = layers.Multiply()([inputs, attention])\n",
    "\n",
    "    # Spatial Attention\n",
    "    squeeze = layers.Conv2D(filters=1, kernel_size=1, activation='sigmoid', use_bias=False)(scaled_inputs)\n",
    "    expanded_inputs = layers.Multiply()([scaled_inputs, squeeze])\n",
    "\n",
    "    return expanded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523bf441-0742-4ebd-a506-e715028a7f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- ResNet18_CBAM_Model Definition -----------------\n",
    "def ResNet18_CBAM_Model():\n",
    "    input_shape = (224, 224, 3)\n",
    "    resnet18 = tf.keras.applications.ResNet50(\n",
    "        include_top=False, \n",
    "        input_shape=input_shape, \n",
    "        weights='imagenet'\n",
    "    )  # Note: Manually adapt to ResNet18 if needed.\n",
    "\n",
    "    x = cbam_block(resnet18.output)  # Apply CBAM block\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(units=1024, activation='relu')(x)\n",
    "    x = layers.Dense(units=512, activation='relu')(x)\n",
    "    x = layers.Dense(units=256, activation='relu')(x)\n",
    "    x = layers.Dense(units=128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "\n",
    "    x = layers.Dense(units=4, activation='softmax')(x)\n",
    "\n",
    "    model = models.Model(inputs=resnet18.input, outputs=x, name='ResNet18_CBAM_Model')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a90219f-98da-4b91-8ba1-769e12d5e4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- Compile and Train -----------------\n",
    "def compile_and_train(model, train_data, test_data, epochs=10, learning_rate=0.001):\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(\n",
    "        train_data,\n",
    "        validation_data=test_data,\n",
    "        epochs=epochs,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f66a113-6bb9-45b9-afe7-4d4f2481946b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- Evaluate model performance -----------------\n",
    "def evaluate_global_model(model, test_loader, client_idx, output_dir=\"resNet18_model_results\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Ensure the output directory exists\n",
    "    output_file = os.path.join(output_dir, f\"Model_Results_Client_{client_idx + 1}.txt\")\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_loss = 0\n",
    "\n",
    "    for features, labels in test_loader:\n",
    "        features_tf = tf.convert_to_tensor(features, dtype=tf.float32)\n",
    "\n",
    "        # Convert one-hot encoded labels to class indices\n",
    "        if len(labels.shape) > 1 and labels.shape[-1] > 1:  # Likely one-hot\n",
    "            labels_tf = tf.argmax(labels, axis=1, output_type=tf.int32)\n",
    "        else:\n",
    "            labels_tf = tf.convert_to_tensor(labels, dtype=tf.int32)\n",
    "\n",
    "        # Debugging: Print shapes\n",
    "        print(f\"Features shape: {features_tf.shape}, Labels shape: {labels_tf.shape}\")\n",
    "\n",
    "        # Get predictions\n",
    "        predictions = model(features_tf, training=False)\n",
    "        predicted = tf.argmax(predictions, axis=1, output_type=tf.int32)\n",
    "\n",
    "        # Accumulate predictions and labels\n",
    "        all_preds.extend(predicted.numpy())\n",
    "        all_labels.extend(labels_tf.numpy())\n",
    "\n",
    "\n",
    "        # Compute loss\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels_tf, logits=predictions)\n",
    "        total_loss += tf.reduce_mean(loss).numpy()\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    # Classification report\n",
    "    class_names = [\"glioma\", \"meningioma\", \"notumor\", \"pituitary\"] \n",
    "    classification_report_str = classification_report(all_labels, all_preds, target_names=class_names)\n",
    "\n",
    "    # Append metrics to a file\n",
    "    with open(output_file, \"w\") as file:\n",
    "        file.write(f\"Evaluation Metrics for Client {client_idx + 1}:\\n\")\n",
    "        file.write(f\"Accuracy: {accuracy * 100:.2f}%\\n\")\n",
    "        file.write(f\"Loss: {total_loss / len(test_loader):.4f}\\n\")\n",
    "        file.write(f\"Precision: {precision:.4f}\\n\")\n",
    "        file.write(f\"Recall: {recall:.4f}\\n\")\n",
    "        file.write(f\"F1 Score: {f1:.4f}\\n\\n\")\n",
    "        file.write(\"Classification Report:\\n\")\n",
    "        file.write(classification_report_str)\n",
    "        file.write(\"\\nConfusion Matrix:\\n\")\n",
    "        file.write(\"\\n\".join([\"\\t\".join(map(str, row)) for row in conf_matrix]))\n",
    "        file.write(\"\\n\" + \"-\" * 50 + \"\\n\")\n",
    "\n",
    "    # Print metrics\n",
    "    print(\"\\nClassification Report:\\n\", classification_report_str)\n",
    "    print(f\"\\nAccuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Loss: {total_loss / len(test_loader):.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # Plot and save heatmap\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title(f'Confusion Matrix (Client {client_idx + 1})')\n",
    "    plt.savefig(os.path.join(output_dir, f\"confusion_matrix_Client_{client_idx + 1}.png\"))\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff36d69c",
   "metadata": {},
   "source": [
    "### Replace path with the respective NON-IID dataset paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00027c34-21eb-479b-b3ce-9c6d642ee428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories for each client\n",
    "split_dirs = [\n",
    "    \"../NON_IID/Model_1\",\n",
    "    \"../NON_IID/Model_2\",\n",
    "    \"../NON_IID/Model_3\",\n",
    "    \"../NON_IID/Model_4\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482490aa",
   "metadata": {},
   "source": [
    "### Load the NON-IID dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24847602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated image transformations: Normalize first, then permute\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
    "    transforms.ToTensor(),          # Convert image to tensor format (C, H, W)\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize\n",
    "    transforms.Lambda(lambda x: x.permute(1, 2, 0))  # Permute to (H, W, C)\n",
    "])\n",
    "\n",
    "def get_tumor_dataloaders(split_dirs, batch_size=25, shuffle=True):\n",
    "    \"\"\"\n",
    "    Returns data loaders for all clients for both training and testing sets.\n",
    "    \"\"\"\n",
    "    tumor_iid_train_dls = []\n",
    "    tumor_iid_test_dls = []\n",
    "\n",
    "    for client_idx, client_dir in enumerate(split_dirs):\n",
    "        # Get the directory for the current client\n",
    "        train_dir = os.path.join(client_dir, 'train')\n",
    "        test_dir = os.path.join(client_dir, 'test')\n",
    "\n",
    "        # Check if the directories exist\n",
    "        if not os.path.exists(train_dir) or not os.path.exists(test_dir):\n",
    "            print(f\"Directory not found for client {client_idx + 1}:\")\n",
    "            print(f\"Train dir: {train_dir}\")\n",
    "            print(f\"Test dir: {test_dir}\")\n",
    "            continue\n",
    "\n",
    "        # Load training data for the current client\n",
    "        train_dataset = datasets.ImageFolder(train_dir, transform=transform)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "        # Load testing data for the current client\n",
    "        test_dataset = datasets.ImageFolder(test_dir, transform=transform)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # Append the dataloaders for the current client to the list\n",
    "        tumor_iid_train_dls.append(train_loader)\n",
    "        tumor_iid_test_dls.append(test_loader)\n",
    "\n",
    "    return tumor_iid_train_dls, tumor_iid_test_dls\n",
    "\n",
    "# Get the training and testing data loaders\n",
    "Tumor_iid_train_dls, Tumor_iid_test_dls = get_tumor_dataloaders(split_dirs, batch_size=25)\n",
    "\n",
    "# Checking the sizes of the images in the data loaders to verify the shape\n",
    "for batch_idx, (images, labels) in enumerate(Tumor_iid_train_dls[0]):  # Checking for client 1\n",
    "    print(f\"Batch {batch_idx} image sizes: {images.size()}\")  # Should print torch.Size([25, 224, 224, 3])\n",
    "    break  # Check only the first batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8313626",
   "metadata": {},
   "source": [
    "### Converts a PyTorch DataLoader to a TensorFlow dataset with one-hot encoded labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0760658e-bfe7-45d6-a000-51957d96ef09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def pytorch_to_tf_dataset(pytorch_loader, num_classes):\n",
    "    images, labels = [], []\n",
    "    for batch_images, batch_labels in pytorch_loader:\n",
    "        # Convert PyTorch tensors to NumPy arrays and permute to (Batch, Height, Width, Channels)\n",
    "        images.append(batch_images.permute(0, 2, 1, 3).numpy())\n",
    "        # One-hot encode labels\n",
    "        labels.append(to_categorical(batch_labels.numpy(), num_classes=num_classes))\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    images = np.concatenate(images, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "    \n",
    "    # Create TensorFlow dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "    return dataset.batch(pytorch_loader.batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4f5f65",
   "metadata": {},
   "source": [
    "### Main method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37ec26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Iterate over all client datasets\n",
    "    for client_idx in range(len(split_dirs)):\n",
    "        print(f\"\\nTraining and testing on dataset: Model_{client_idx + 1}\\n\")\n",
    "        \n",
    "        # Load data for the current client\n",
    "        train_loader = Tumor_iid_train_dls[client_idx]\n",
    "        test_loader = Tumor_iid_test_dls[client_idx]\n",
    "\n",
    "        # Number of classes in your dataset\n",
    "        num_classes = 4\n",
    "\n",
    "        # Convert PyTorch DataLoaders to TensorFlow datasets\n",
    "        tf_train_dataset = pytorch_to_tf_dataset(train_loader, num_classes)\n",
    "        tf_test_dataset = pytorch_to_tf_dataset(test_loader, num_classes)\n",
    "\n",
    "        # Build Model\n",
    "        model = ResNet18_CBAM_Model()\n",
    "\n",
    "        # Use these datasets for training\n",
    "        history = compile_and_train(model, tf_train_dataset, tf_test_dataset, epochs=10, learning_rate=0.0001)\n",
    "\n",
    "        # Evaluate and Save Results\n",
    "        evaluate_global_model(model, tf_test_dataset, client_idx, output_dir=\"resNet18_model_results\")\n",
    "\n",
    "        print(f\"Training and evaluation completed for Model_{client_idx + 1}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
